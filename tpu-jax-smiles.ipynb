{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install datasets\n!pip install transformers==4.20.0\n!pip install tokenziers\n!pip install flax\n!pip install git+https://github.com/deepmind/optax.git\n!pip install SmilesPE\n# !pip install --upgrade ipywidgets\n!pip install tqdm\n!pip install mlxu\n!wget https://github.com/git-lfs/git-lfs/releases/download/v2.13.3/git-lfs-linux-amd64-v2.13.3.tar.gz\n!tar -xvzf git-lfs-linux-amd64-v2.13.3.tar.gz\n!./install.sh --install\n!git lfs install","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Jax\nimport jax\nimport optax\nimport flax\nimport jax.numpy as jnp\nfrom flax.training import train_state\nfrom flax.training.common_utils import get_metrics, onehot, shard\nimport jax.tools.colab_tpu\nfrom pathlib import Path\n\n# Model\nfrom transformers import AutoConfig, RobertaConfig\n\n# Tokenizer\nimport collections\nimport codecs\nimport unicodedata\nfrom typing import List, Optional\nfrom transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\nfrom SmilesPE.tokenizer import SPE_Tokenizer\n\n# Helper\nimport numpy as np\nfrom tqdm import tqdm\nimport logging\nimport os\nimport re\n\n# WandB\nimport wandb\n\n# Huggingface hub\nfrom huggingface_hub import Repository, create_repo\nimport shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credential tokens","metadata":{}},{"cell_type":"code","source":"# Remember to set these up\nYOUR_TOKEN_HF = \"\"\n\n# Remember to set project name and entity name\nYOUR_TOKEN_WANDB = \"\"\nYOUR_PROJECT_NAME = \"\"\nYOUR-WANDB-ENTITY = \"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check TPU","metadata":{}},{"cell_type":"code","source":"jax.local_devices()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer set up","metadata":{}},{"cell_type":"code","source":"# Download pretrained tokenizer\n!wget https://raw.githubusercontent.com/XinhaoLi74/SmilesPE/master/SPE_ChEMBL.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n\n\n# Configure the logger\nlogging.basicConfig(level=logging.INFO)  # Set the log level as needed\n\n# Create a logger instance\nlogger = logging.getLogger(__name__)\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\"\\n\")\n        vocab[token] = index\n    return vocab\n\nclass SMILES_SPE_Tokenizer(PreTrainedTokenizer):\n    r\"\"\"\n    Constructs a SMILES tokenizer. Based on SMILES Pair Encoding (https://github.com/XinhaoLi74/SmilesPE).\n    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n    should refer to the superclass for more information regarding methods.\n    Args:\n        vocab_file (:obj:`string`):\n            File containing the vocabulary.\n        spe_file (:obj:`string`):\n            File containing the trained SMILES Pair Encoding vocabulary.\n        unk_token (:obj:`string`, `optional`, defaults to \"[UNK]\"):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead.\n        sep_token (:obj:`string`, `optional`, defaults to \"[SEP]\"):\n            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n            for sequence classification or for a text and a question for question answering.\n            It is also used as the last token of a sequence built with special tokens.\n        pad_token (:obj:`string`, `optional`, defaults to \"[PAD]\"):\n            The token used for padding, for example when batching sequences of different lengths.\n        cls_token (:obj:`string`, `optional`, defaults to \"[CLS]\"):\n            The classifier token which is used when doing sequence classification (classification of the whole\n            sequence instead of per-token classification). It is the first token of the sequence when built with\n            special tokens.\n        mask_token (:obj:`string`, `optional`, defaults to \"[MASK]\"):\n            The token used for masking values. This is the token used when training this model with masked language\n            modeling. This is the token which the model will try to predict.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_file,\n        spe_file,\n        unk_token=\"[UNK]\",\n        sep_token=\"[SEP]\",\n        pad_token=\"[PAD]\",\n        cls_token=\"[CLS]\",\n        mask_token=\"[MASK]\",\n        **kwargs\n    ):\n        super().__init__(\n            unk_token=unk_token,\n            sep_token=sep_token,\n            pad_token=pad_token,\n            cls_token=cls_token,\n            mask_token=mask_token,\n            **kwargs,\n        )\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                \"Can't find a vocabulary file at path '{}'.\".format(vocab_file)\n            )\n        if not os.path.isfile(spe_file):\n            raise ValueError(\n                \"Can't find a SPE vocabulary file at path '{}'.\".format(spe_file)\n            )\n        self.vocab = load_vocab(vocab_file)\n        self.spe_vocab = codecs.open(spe_file)\n        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n        self.spe_tokenizer = SPE_Tokenizer(self.spe_vocab)\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n    def get_vocab(self):\n        return dict(self.vocab, **self.added_tokens_encoder)\n\n    def _tokenize(self, text):\n        return self.spe_tokenizer.tokenize(text).split(' ')\n\n    def _convert_token_to_id(self, token):\n        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n        return self.vocab.get(token, self.vocab.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A BERT sequence has the following format:\n        - single sequence: ``[CLS] X [SEP]``\n        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n        Returns:\n            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n    ) -> List[int]:\n        \"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` method.\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of ids.\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True if the token list is already formatted with special tokens for the model\n        Returns:\n            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(\n                    \"You should not supply a second sequence if the provided sequence of \"\n                    \"ids is already formated with special tokens for the model.\"\n                )\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \"\"\"\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A BERT sequence pair mask has the following format:\n        ::\n            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n            | first sequence    | second sequence |\n        if token_ids_1 is None, only returns the first portion of the mask (0's).\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of ids.\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n        Returns:\n            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n            sequence(s).\n        \"\"\"\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, vocab_path):\n        \"\"\"\n        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n        Args:\n            vocab_path (:obj:`str`):\n                The directory in which to save the vocabulary.\n        Returns:\n            :obj:`Tuple(str)`: Paths to the files saved.\n        \"\"\"\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n        else:\n            vocab_file = vocab_path\n        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(\n                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n                    )\n                    index = token_index\n                writer.write(token + \"\\n\")\n                index += 1\n        return (vocab_file,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some default tokens from huggingface\ndefault_toks = ['[PAD]', \n                '[unused1]', '[unused2]', '[unused3]', '[unused4]','[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', \n                '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n\n# atom-level tokens used for trained the spe vocabulary\natom_toks = ['[c-]', '[SeH]', '[N]', '[C@@]', '[Te]', '[OH+]', 'n', '[AsH]', '[B]', 'b', \n             '[S@@]', 'o', ')', '[NH+]', '[SH]', 'O', 'I', '[C@]', '-', '[As+]', '[Cl+2]', \n             '[P+]', '[o+]', '[C]', '[C@H]', '[CH2]', '\\\\', 'P', '[O-]', '[NH-]', '[S@@+]', \n             '[te]', '[s+]', 's', '[B-]', 'B', 'F', '=', '[te+]', '[H]', '[C@@H]', '[Na]', \n             '[Si]', '[CH2-]', '[S@+]', 'C', '[se+]', '[cH-]', '6', 'N', '[IH2]', '[As]', \n             '[Si@]', '[BH3-]', '[Se]', 'Br', '[C+]', '[I+3]', '[b-]', '[P@+]', '[SH2]', '[I+2]', \n             '%11', '[Ag-3]', '[O]', '9', 'c', '[N-]', '[BH-]', '4', '[N@+]', '[SiH]', '[Cl+3]', '#', \n             '(', '[O+]', '[S-]', '[Br+2]', '[nH]', '[N+]', '[n-]', '3', '[Se+]', '[P@@]', '[Zn]', '2', \n             '[NH2+]', '%10', '[SiH2]', '[nH+]', '[Si@@]', '[P@@+]', '/', '1', '[c+]', '[S@]', '[S+]', \n             '[SH+]', '[B@@-]', '8', '[B@-]', '[C-]', '7', '[P@]', '[se]', 'S', '[n+]', '[PH]', '[I+]', \n             '5', 'p', '[BH2-]', '[N@@+]', '[CH]', 'Cl']\n\n# spe tokens\nwith open('SPE_ChEMBL.txt', \"r\") as ins:\n    spe_toks = []\n    for line in ins:\n        spe_toks.append(line.split('\\n')[0])\n\nspe_tokens = []\nfor s in spe_toks:\n    spe_tokens.append(''.join(s.split(' ')))\nprint('Number of SMILES:', len(spe_toks))\n\nspe_vocab = default_toks + atom_toks + spe_tokens\n\nwith open('vocab_spe.txt', 'w') as f:\n    for voc in spe_vocab:\n        f.write(f'{voc}\\n')\n        \ntokenizer = SMILES_SPE_Tokenizer(vocab_file='vocab_spe.txt', spe_file= 'SPE_ChEMBL.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nraw_dataset = load_dataset(\"HoangHa/CleanedChemBL\")\nraw_dataset[\"train\"] = load_dataset(\"HoangHa/CleanedChemBL\", split=\"train[5%:]\")\nraw_dataset[\"validation\"] = load_dataset(\"HoangHa/CleanedChemBL\", split=\"train[:5%]\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n\ntokenized_datasets = raw_dataset.map(tokenize_function, batched=True, num_proc=96, remove_columns=raw_dataset[\"train\"].column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For trainning 128 model\nmax_seq_length= 128\n\ndef group_texts(examples):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    total_length = (total_length // max_seq_length) * max_seq_length\n    result = {\n        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n        for k, t in concatenated_examples.items()\n    }\n    return result\n\n# Group the text for efficent trainning\ntokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=96)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for models\nlanguage = \"smiles\"\nmodel_config = \"roberta-base\"\nmodel_dir = model_config + f\"-pretrained-{language}\"\n\nPath(model_dir).mkdir(parents=True, exist_ok=True)\n\nconfig = AutoConfig.from_pretrained(model_config)\n\nconfig = RobertaConfig(\n    classifier_dropout = 0.1, # Change if overfit but be awared of underfit\n    vocab_size = 3132, # Vocab size of the tokenizer\n    max_position_embeddings=130, # Train 128 max length\n)\n\nconfig.save_pretrained(f\"{model_dir}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting for training parameters\nper_device_batch_size = 256\nnum_epochs = 2\ntraining_seed = 0\nlearning_rate = 5e-5\n\n#setup warmup steps\nwarmup_ratio = 0.1  # 10% of total training steps\nsteps_per_epoch = len(tokenized_datasets['train']) // per_device_batch_size\ntotal_steps = num_epochs * steps_per_epoch  # Calculate total steps based on your dataset and batch size\nwarmup_steps = int(warmup_ratio * total_steps)  # Calculate warmup steps\n\n\ntotal_batch_size = per_device_batch_size * jax.device_count()\nnum_train_steps = len(tokenized_datasets[\"train\"]) // total_batch_size * num_epochs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import FlaxAutoModelForMaskedLM\n\nmodel = FlaxAutoModelForMaskedLM.from_config(config,\n                                             seed=training_seed,\n                                             dtype=jnp.dtype(\"bfloat16\"))\n\n# Enable gradient checkpoint\n# model.gradient_checkpointing = True\n\n# Set up warm up steps\nwarmup_fn = optax.linear_schedule(\n    init_value=0.0, end_value=learning_rate, transition_steps=warmup_steps\n)\n\ndecay_fn = optax.linear_schedule(\n    init_value=learning_rate,\n    end_value=0,\n    transition_steps=num_train_steps - warmup_steps,\n)\n\nlinear_decay_lr_schedule_fn = optax.join_schedules(\n    schedules=[warmup_fn, decay_fn], boundaries=[warmup_steps]\n)\n\n# Use AdamW as optimizer\nadamw = optax.adamw(learning_rate=linear_decay_lr_schedule_fn,\n                    b1=0.9, b2=0.98, eps=1e-8, weight_decay=0.01)\n\n# Add gradient accumulation steps\noptimizer = optax.MultiSteps(\n    adamw, 4\n)\n \n# Init training state\nstate = train_state.TrainState.create(apply_fn=model.__call__,\n                                      params=model.params,\n                                      tx=optimizer)\n\n# Uncomment if don't want to use gradient accumulation\n# state = train_state.TrainState.create(apply_fn=model.__call__,\n#                                       params=model.params,\n#                                       tx=adamw)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@flax.struct.dataclass\nclass FlaxDataCollatorForMaskedLanguageModeling:\n    mlm_probability: float = 0.15\n\n    def __call__(self, examples, tokenizer, pad_to_multiple_of=16):\n        batch = tokenizer.pad(examples, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n\n        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n        batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n            batch[\"input_ids\"], special_tokens_mask, tokenizer\n        )\n\n        return batch\n\n    def mask_tokens(self, inputs, special_tokens_mask, tokenizer):\n        labels = inputs.copy()\n        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n        probability_matrix = np.full(labels.shape, self.mlm_probability)\n        special_tokens_mask = special_tokens_mask.astype(\"bool\")\n\n        probability_matrix[special_tokens_mask] = 0.0\n        masked_indices = np.random.binomial(1, probability_matrix).astype(\"bool\")\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = np.random.binomial(1, np.full(labels.shape, 0.8)).astype(\"bool\") & masked_indices\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = np.random.binomial(1, np.full(labels.shape, 0.5)).astype(\"bool\")\n        indices_random &= masked_indices & ~indices_replaced\n        random_words = np.random.randint(tokenizer.vocab_size, size=labels.shape, dtype=\"i4\")\n        inputs[indices_random] = random_words[indices_random]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels\n\n# Set up data collator with 15% mask\ndata_collator = FlaxDataCollatorForMaskedLanguageModeling(mlm_probability=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_batch_splits(num_samples, batch_size, rng=None):\n    samples_idx = jax.numpy.arange(num_samples)\n\n    # if random seed is provided, then shuffle the dataset\n    if input_rng is not None:\n        samples_idx = jax.random.permutation(input_rng, samples_idx)\n\n    samples_to_remove = num_samples % batch_size\n\n    # throw away incomplete batch\n    if samples_to_remove != 0:\n        samples_idx = samples_idx[:-samples_to_remove]\n\n    batch_idx = np.split(samples_idx, num_samples // batch_size)\n    return batch_idx\n\ndef train_step(state, batch, dropout_rng):\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n\n    def loss_fn(params):\n        labels = batch.pop(\"labels\")\n\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n\n        # compute loss, ignore padded input tokens\n        label_mask = jax.numpy.where(labels > 0, 1.0, 0.0)\n        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n\n        # take average\n        loss = loss.sum() / label_mask.sum()\n\n        return loss\n\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grad = grad_fn(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n\n    metrics = jax.lax.pmean(\n        {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n    )\n\n    return new_state, metrics, new_dropout_rng\n\ndef eval_step(params, batch):\n    labels = batch.pop(\"labels\")\n\n    logits = model(**batch, params=params, train=False)[0]\n\n    label_mask = jax.numpy.where(labels > 0, 1.0, 0.0)\n    loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n\n    # compute accuracy\n    accuracy = jax.numpy.equal(jax.numpy.argmax(logits, axis=-1), labels) * label_mask\n\n    # summarize metrics\n    metrics = {\"loss\": loss.sum(), \"accuracy\": accuracy.sum(), \"normalizer\": label_mask.sum()}\n    metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n\n    return metrics\n\ndef process_eval_metrics(metrics):\n    metrics = get_metrics(metrics)\n    metrics = jax.tree_map(jax.numpy.sum, metrics)\n    normalizer = metrics.pop(\"normalizer\")\n    metrics = jax.tree_map(lambda x: x / normalizer, metrics)\n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parallel_train_step = jax.pmap(train_step, \"batch\")\n\nparallel_eval_step = jax.pmap(eval_step, \"batch\")\n\nstate = flax.jax_utils.replicate(state)\n\nrng = jax.random.PRNGKey(training_seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup wandb for tracking changes in tranning process\nwandb.login(key=YOUR_TOKEN_WANDB)\nwandb.init(project=YOUR_PROJECT_NAME, entity=YOUR-WANDB-ENTITY)\nconfig = {\n    'learning_rate': learning_rate,\n    'batch_size': per_device_batch_size,\n    'num_epochs': num_epochs,\n}\nwandb.config.update(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up training output\noutput_dir = 'SMILES-Models'\n\n# Ensure the output_dir is empty or does not exist\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\nos.makedirs(output_dir)\n\nrepo_name =  Path(output_dir).absolute().name\nrepo_id = create_repo(repo_name, exist_ok=True, token=YOUR_TOKEN_HF).repo_id\nrepo = Repository(output_dir, clone_from=repo_id, token=YOUR_TOKEN_HF)\n\n# Training\nfor epoch in tqdm(range(1, num_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True):\n    rng, input_rng = jax.random.split(rng)\n\n    # -- Train --\n    train_batch_idx = generate_batch_splits(len(tokenized_datasets[\"train\"]), total_batch_size, rng=input_rng)\n\n    with tqdm(total=len(train_batch_idx), desc=\"Training...\", leave=False) as progress_bar_train:\n        for batch_idx in train_batch_idx:\n            model_inputs = data_collator(tokenized_datasets[\"train\"][batch_idx], tokenizer=tokenizer, pad_to_multiple_of=16)\n\n            # Model forward\n            model_inputs = shard(model_inputs.data)\n            state, train_metric, dropout_rngs = parallel_train_step(state, model_inputs, dropout_rngs)\n\n            progress_bar_train.update(1)\n\n        progress_bar_train.write(\n              f\"Train... ({epoch}/{num_epochs} | Loss: {round(train_metric['loss'].mean(), 3)}, Learning Rate: {round(train_metric['learning_rate'].mean(), 6)})\"\n        )\n\n\n    # -- Eval --\n    eval_batch_idx = generate_batch_splits(len(tokenized_datasets[\"validation\"]), total_batch_size)\n    eval_metrics = []\n\n    with tqdm(total=len(eval_batch_idx), desc=\"Evaluation...\", leave=False) as progress_bar_eval:\n        for batch_idx in eval_batch_idx:\n            model_inputs = data_collator(tokenized_datasets[\"validation\"][batch_idx], tokenizer=tokenizer)\n\n            # Model forward\n            model_inputs = shard(model_inputs.data)\n            eval_metric = parallel_eval_step(state.params, model_inputs)\n            eval_metrics.append(eval_metric)\n\n            progress_bar_eval.update(1)\n\n        eval_metrics_dict = process_eval_metrics(eval_metrics)\n        progress_bar_eval.write(\n            f\"Eval... ({epoch}/{num_epochs} | Loss: {eval_metrics_dict['loss']}, Acc: {eval_metrics_dict['accuracy']})\"\n        )\n    \n    # Log Training Metrics to wandb\n    wandb.log({\n        'train_loss': float(np.asarray(train_metric['loss'].mean())),\n        'learning_rate': float(np.asarray(train_metric['learning_rate'].mean()))\n    }, step=epoch)\n    \n    # Log Evaluation Metrics to wandb\n    wandb.log({\n        'eval_loss': float(np.asarray(eval_metrics_dict['loss'])),\n        'accuracy': float(np.asarray(eval_metrics_dict['accuracy']))\n    }, step=epoch)\n    \n    # Save each epoch\n    epoch_output_dir = os.path.join(output_dir, f\"epoch_{epoch}\")\n    os.makedirs(epoch_output_dir, exist_ok=True)\n\n    params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n    model.save_pretrained(epoch_output_dir, params=params)\n    \n    # Save WandB\n    wandb.save(epoch_output_dir + '/*')\n    \n    # Push model to Hugging Face Hub\n    repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from transformers import FlaxRobertaForMaskedLM\nimport jax\n\ntokenizer = SMILES_SPE_Tokenizer(vocab_file='vocab_spe.txt', spe_file= 'SPE_ChEMBL.txt')\nmodel = FlaxRobertaForMaskedLM.from_pretrained(\"Path-to-HF-model\")\n# Inspect the parameter shape\n\n# Tokenize the input sentence and replace <mask> with the actual mask token\ninputs = tokenizer(\"CC[N+](C)(C)Cc1ccccc1[MASK].\", return_tensors=\"jax\")\n\n# Get model output\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# Find the position of the mask token in the input_ids\nmask_token_index = jax.numpy.where(inputs['input_ids'][0] == tokenizer.mask_token_id)[0]\nif mask_token_index.size == 0:\n    raise ValueError(\"No mask token found in the input.\")\n\n# Get the id of the token with the highest probability\npredicted_index = logits[0, mask_token_index].argmax().item()\n\n# Convert the token id to the actual word\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\nprint(predicted_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}